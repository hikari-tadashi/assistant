Use 'ollama_single_chat' for the quickest local start

TODO:? 
Make Ollama plugin a standalone? plugin? treat LLM generation as a black box?
- I am to lazy to deal with streaming